import json
import pytest
import uuid
from distutils.version import LooseVersion as LV
from calm.dsl.cli.main import get_api_client
from calm.dsl.cli.constants import RUNLOG
from calm.dsl.runbooks import read_local_file
from tests.api_interface.test_runbooks.test_files.decision_task import (
    DecisionTask,
)
from utils import upload_runbook, poll_runlog_status
from calm.dsl.store import Version

DSL_CONFIG = json.loads(read_local_file(".tests/config.json"))
linux_ip2 = DSL_CONFIG["EXISTING_MACHINE"]["IP_2"]

CALM_VERSION = Version.get_version("Calm")


class TestDecisionTasks:
    @pytest.mark.runbook
    @pytest.mark.ces
    @pytest.mark.regression
    def test_decision_task(self):
        """test_desision_task"""

        client = get_api_client()
        rb_name = "test_decisiontask_" + str(uuid.uuid4())[-10:]

        rb = upload_runbook(client, rb_name, DecisionTask)
        rb_state = rb["status"]["state"]
        rb_uuid = rb["metadata"]["uuid"]
        print(">> Runbook state: {}".format(rb_state))
        assert rb_state == "ACTIVE"
        assert rb_name == rb["spec"]["name"]
        assert rb_name == rb["metadata"]["name"]

        # endpoints generated by this runbook
        endpoint_list = rb["spec"]["resources"].get("endpoint_definition_list", [])

        # running the runbook
        print("\n>>Running the runbook")

        res, err = client.runbook.run(rb_uuid, {})
        if err:
            pytest.fail("[{}] - {}".format(err["code"], err["error"]))

        response = res.json()
        runlog_uuid = response["status"]["runlog_uuid"]

        # polling till runbook run gets to terminal state
        state, reasons = poll_runlog_status(client, runlog_uuid, RUNLOG.TERMINAL_STATES)

        print(">> Runbook Run state: {}\n{}".format(state, reasons))
        assert state == RUNLOG.STATUS.SUCCESS

        # Check if correct SUCCESS/FAILURE PATHS are taken
        IncorrectPaths = [
            "SUCCESS2",
            "FAILURE1",
            "SUCCESS4",
            "FAILURE3",
            "SUCCESS6",
            "FAILURE5",
        ]
        res, err = client.runbook.list_runlogs(runlog_uuid)
        if err:
            pytest.fail("[{}] - {}".format(err["code"], err["error"]))
        response = res.json()
        entities = response["entities"]
        for entity in entities:
            if (
                entity["status"]["type"] == "task_runlog"
                and entity["status"]["task_reference"]["name"] in IncorrectPaths
            ):
                pytest.fail(
                    "[{}] path should not get executed".format(
                        entity["status"]["task_reference"]["name"]
                    )
                )

        # delete the runbook
        _, err = client.runbook.delete(rb_uuid)
        if err:
            pytest.fail("[{}] - {}".format(err["code"], err["error"]))
        else:
            print("runbook {} deleted".format(rb_name))

        # delete endpoints generated by this test
        for endpoint in endpoint_list:
            _, err = client.endpoint.delete(endpoint["uuid"])
            if err:
                pytest.fail("[{}] - {}".format(err["code"], err["error"]))

    @pytest.mark.runbook
    @pytest.mark.ces
    @pytest.mark.regression
    @pytest.mark.skipif(
        LV(CALM_VERSION) < LV("3.5.0"),
        reason="Inherit target from parent supported from 3.5.0",
    )
    def test_desision_task_with_multiple_targets(self):
        """test_desision_task_with_multiple_targets"""
        # inline import is needed so that version check happens before creating runbook
        from tests.api_interface.test_runbooks.test_files.decision_task_with_multiple_target import (
            DecisionWithMultipleIpTarget,
        )

        global linux_ip2
        client = get_api_client()
        rb_name = "test_decisiontask_" + str(uuid.uuid4())[-10:]

        rb = upload_runbook(client, rb_name, DecisionWithMultipleIpTarget)
        rb_state = rb["status"]["state"]
        rb_uuid = rb["metadata"]["uuid"]
        print(">> Runbook state: {}".format(rb_state))
        assert rb_state == "ACTIVE"
        assert rb_name == rb["spec"]["name"]
        assert rb_name == rb["metadata"]["name"]

        # endpoints generated by this runbook
        endpoint_list = rb["spec"]["resources"].get("endpoint_definition_list", [])

        # running the runbook
        print("\n>>Running the runbook")

        res, err = client.runbook.run(rb_uuid, {})
        if err:
            pytest.fail("[{}] - {}".format(err["code"], err["error"]))

        response = res.json()
        runlog_uuid = response["status"]["runlog_uuid"]

        # polling till runbook run gets to terminal state
        state, reasons = poll_runlog_status(client, runlog_uuid, RUNLOG.TERMINAL_STATES)

        print(">> Runbook Run state: {}\n{}".format(state, reasons))
        assert state == RUNLOG.STATUS.SUCCESS

        res, err = client.runbook.list_runlogs(runlog_uuid)
        if err:
            pytest.fail("[{}] - {}".format(err["code"], err["error"]))
        response = res.json()

        linux_ip1 = read_local_file(".tests/runbook_tests/vm_ip")

        entities = response["entities"]
        task_machine_pair = {
            "SUCCESS1_D1": linux_ip1,
            "FAILURE1_D1": linux_ip2,
            "SUCCESS1_D2": linux_ip2,
        }
        validated_task_list = []
        for entity in entities:
            task_type = entity["status"]["type"]
            if task_type == "task_runlog":
                task_name = entity["status"]["task_reference"]["name"]
                if task_name == "FAILURE1_D2":
                    pytest.fail("[{}] path should not get executed".format(task_name))
                elif task_name in task_machine_pair.keys():
                    assert (
                        task_machine_pair.get(task_name)
                        in entity["status"]["machine_name"]
                    ), "Task {} is not executed on expected machine {}".format(
                        task_name, task_machine_pair.get(task_name)
                    )
                    validated_task_list.append(task_name)
        expected_tasks = list(task_machine_pair.keys())
        assert (
            validated_task_list.sort() == expected_tasks.sort()
        ), "Expected path was not hit"
